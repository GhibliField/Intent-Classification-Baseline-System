{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "### 文本表示\n",
    "n-gram + BoW\n",
    "### 分类器\n",
    "NBSVM是Sida Wang 和 Chris Manning 在其论文 [Baselines and Bigrams: Simple, Good Sentiment and Topic Classiﬁcation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)中提出的. 由于在实践中，svm和逻辑回归十分接近，本文直接使用逻辑回归代替SVM。\n",
    "If you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https://youtu.be/37sFIak42Sc?t=3745)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855",
    "nbpresent": {
     "id": "7b259f33-a152-48d9-9a0e-89df65ab1686"
    }
   },
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "from __future__ import print_function\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.utils.extmath import density\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.preprocessing import scale\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import scipy\n",
    "import pickle\n",
    "import json\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import six\n",
    "from abc import ABCMeta\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils import check_X_y, check_array\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction import text\n",
    "from classifiers import NBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../data/'\n",
    "pkls='../models/'\n",
    "if not os.path.exists(pkls):\n",
    "    os.mkdir(pkls)\n",
    "stopwords_path='../utils/stopword_2792.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_contents=[]\n",
    "# tra_ids=[]\n",
    "tra_labels=[]\n",
    "with open(f'{data}train.json','r') as tra_f:\n",
    "    for idx,each in enumerate(tra_f):\n",
    "        samp=json.loads(each.strip())\n",
    "        label=samp['label']#标签\n",
    "        content=samp['text']#内容\n",
    "\n",
    "        tra_labels.append(label)\n",
    "        tra_contents.append(content)\n",
    "\n",
    "train_set={'content':tra_contents,\n",
    "           'label':tra_labels}\n",
    "\n",
    "train_df=pd.DataFrame(train_set)\n",
    "print('Trainset Loaded')\n",
    "\n",
    "\n",
    "val_contents=[]\n",
    "# val_ids=[]\n",
    "val_labels=[]\n",
    "with open(f'{data}devel.json','r') as val_f:\n",
    "    for idx,each in enumerate(val_f):\n",
    "        samp=json.loads(each.strip())\n",
    "        label=samp['label']#标签\n",
    "        content=samp['text']#内容\n",
    "\n",
    "        val_labels.append(label)\n",
    "        val_contents.append(content)\n",
    "\n",
    "val_set={'content':val_contents,\n",
    "           'label':val_labels}\n",
    "\n",
    "val_df=pd.DataFrame(val_set)\n",
    "print('Val-set Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#停用词\n",
    "with open(stopwords_path,'r') as stw:\n",
    "    stopwords=[x.strip() for x in stw]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "del stopwords\n",
    "\n",
    "x_train = train_df['content']\n",
    "y_train= pd.Series(train_df['label'])\n",
    "x_val = val_df['content']\n",
    "y_val= pd.Series(val_df['label'])\n",
    "x_dataset=pd.concat([x_train,x_val],axis=0,ignore_index=True)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=my_stop_words,\n",
    "    ngram_range=(1, 2))\n",
    "word_vectorizer.fit(x_dataset)\n",
    "x_wd = word_vectorizer.transform(x_train)\n",
    "joblib.dump(word_vectorizer,f'{pkls}nbsvm-vocab-wd.pkl')\n",
    "\n",
    "# 使用字符特征\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "    stop_words=my_stop_words,\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 3))\n",
    "char_vectorizer.fit(x_dataset)\n",
    "# char_vectorizer=joblib.load(f'{pkls}vocab-ch.pkl')\n",
    "x_ch = char_vectorizer.transform(x_train)\n",
    "\n",
    "joblib.dump(char_vectorizer,f'{pkls}nbsvm-vocab-ch.pkl')\n",
    "x_train = hstack([x_ch, x_wd])\n",
    "del x_ch\n",
    "del x_wd\n",
    "x_train=scipy.sparse.csr_matrix(x_train)\n",
    "print('training data vectorized')\n",
    "\n",
    "\n",
    "x_val_wd = word_vectorizer.transform(x_val)\n",
    "\n",
    "x_val_ch = char_vectorizer.transform(x_val)\n",
    "x_val = hstack([x_val_ch, x_val_wd])\n",
    "del x_val_ch\n",
    "del x_val_wd\n",
    "x_val=scipy.sparse.csr_matrix(x_val)\n",
    "print('develope data vectorized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading...')\n",
    "clf = NBSVM()\n",
    "\n",
    "ch2 = SelectKBest(chi2, k=4000)\n",
    "x_train = ch2.fit_transform(x_train,tra_labels)\n",
    "print('fitting...')\n",
    "clf.fit(x_train,tra_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型持久化\n",
    "joblib.dump(clf,f'{pkls}nbsvm_31.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(ch2,f'{pkls}nbsvm-feature_selector.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_result(actual, pred):\n",
    "    print('predict info:')\n",
    "    print('f1-score:{0:.3f}'.format(metrics.f1_score(actual, pred,average='macro',labels=np.unique(pred))))\n",
    "    print(\"accuracy:   %0.3f\" % metrics.accuracy_score(actual, pred))\n",
    "    print(metrics.classification_report(actual,pred,target_names=list(set(tra_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在开发集上测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing...')\n",
    "x_val = ch2.transform(x_val)#特征选择\n",
    "pred = clf.predict(x_val)\n",
    "\n",
    "evaluation_result(y_val,pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a99c4d9-916f-4189-9a25-fedcb7700336",
    "_uuid": "5525045116474e6d12b6edc890250d30c0790f06",
    "nbpresent": {
     "id": "2ed5aa68-d75f-4e7a-bbb2-b5982fc1e8b1"
    }
   },
   "source": [
    "# 3 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jieba\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from classifiers import NBSVM\n",
    "\n",
    "pkls='../models/'\n",
    "        \n",
    "def onlineTest(raw_query):\n",
    "    text = [''.join([w for w in jieba.cut(raw_query)])]\n",
    "    char_vectorizer_=joblib.load(f'{pkls}nbsvm-vocab-ch.pkl')\n",
    "    test_ch = char_vectorizer_.transform(text)\n",
    "    word_vectorizer_=joblib.load(f'{pkls}nbsvm-vocab-wd.pkl')\n",
    "    test_wd = word_vectorizer_.transform(text)\n",
    "    test_vec=hstack([test_ch, test_wd])\n",
    "    test_vec=csr_matrix(test_vec)\n",
    "\n",
    "    clf_=NBSVM()\n",
    "    clf_=joblib.load(f'{pkls}nbsvm_31.pkl')\n",
    "    ch2_=joblib.load(f'{pkls}nbsvm-feature_selector.pkl')\n",
    "    test_vec = ch2_.transform(test_vec)\n",
    "    pred=clf_.predict(test_vec)\n",
    "    return pred.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlineTest('我想听一首王源的做我自己')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
