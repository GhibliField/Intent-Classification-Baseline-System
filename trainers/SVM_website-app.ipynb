{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.extmath import density\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import jieba\n",
    "from sklearn.preprocessing import scale\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import scipy\n",
    "import pickle\n",
    "import json\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '../data/'\n",
    "pkls='../models/'\n",
    "if not os.path.exists(pkls):\n",
    "    os.mkdir(pkls)\n",
    "stopwords_path='../utils/stopword_2792.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tra_contents=[]\n",
    "tra_ids=[]\n",
    "tra_labels=[]\n",
    "with open(f'{data}train.json','r') as tra_f:\n",
    "    for idx,each in enumerate(tra_f):\n",
    "        samp=json.loads(each.strip())\n",
    "        label=samp['label']#标签\n",
    "        if label in ['website','app']:\n",
    "            content=samp['text']#内容\n",
    "\n",
    "            tra_labels.append(label)\n",
    "            tra_contents.append(content)\n",
    "\n",
    "train_set={'content':tra_contents,\n",
    "           'label':tra_labels}\n",
    "\n",
    "train_df=pd.DataFrame(train_set)\n",
    "print('Trainset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contents=[]\n",
    "val_labels=[]\n",
    "with open(f'{data}devel.json','r') as val_f:\n",
    "    for idx,each in enumerate(val_f):\n",
    "        samp=json.loads(each.strip())\n",
    "        label=samp['label']#标签\n",
    "        if label in ['website','app']:\n",
    "            content=samp['text']#内容\n",
    "\n",
    "            val_labels.append(label)\n",
    "            val_contents.append(content)\n",
    "\n",
    "val_set={'content':val_contents,\n",
    "           'label':val_labels}\n",
    "\n",
    "val_df=pd.DataFrame(val_set)\n",
    "print('Val-set Loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhfont = mpl.font_manager.FontProperties(fname='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc')\n",
    "#绘制柱状图\n",
    "\n",
    "label2freq = {}\n",
    "\n",
    "for label in labels:\n",
    "    label2freq[label] = len(train_df.loc[train_df['label']==label])\n",
    "#创建柱状图\n",
    "#第一个参数为柱的横坐标\n",
    "#第二个参数为柱的高度\n",
    "#参数align为柱的对齐方式，以第一个参数为参考标准\n",
    "pyplot.bar(range(2), [label2freq.get(label, 0) for label in labels], align='center')\n",
    "\n",
    "#设置柱的文字说明\n",
    "#第一个参数为文字说明的横坐标\n",
    "#第二个参数为文字说明的内容\n",
    "pyplot.xticks(range(2), labels,fontproperties=zhfont)\n",
    "\n",
    "#设置横坐标的文字说明\n",
    "pyplot.xlabel('类别',fontproperties=zhfont)\n",
    "#设置纵坐标的文字说明\n",
    "pyplot.ylabel('样本个数')\n",
    "#设置标题\n",
    "pyplot.title('样本分布',fontproperties=zhfont)\n",
    "#绘图\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 生成词袋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词\n",
    "with open(stopwords_path,'r') as stw:\n",
    "    stopwords=[x.strip() for x in stw]\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "del stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ = train_df['content']\n",
    "x_val_ = val_df['content']\n",
    "x_dataset=pd.concat([x_train_,x_val_],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words=my_stop_words,\n",
    "    ngram_range=(1, 2))\n",
    "word_vectorizer.fit(x_dataset)\n",
    "\n",
    "# 使用字符特征\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "#     strip_accents='unicode',\n",
    "    stop_words=my_stop_words,\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 3))\n",
    "char_vectorizer.fit(x_dataset)\n",
    "\n",
    "\n",
    "joblib.dump(word_vectorizer,f'{pkls}vocab-wd_website-app.pkl')\n",
    "joblib.dump(char_vectorizer,f'{pkls}vocab-ch_website-app.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 文本表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#训练数据\n",
    "\n",
    "y_train_= pd.Series(train_df['label'])\n",
    "x_wd = word_vectorizer.transform(x_train_)\n",
    "x_ch = char_vectorizer.transform(x_train_)\n",
    "x = hstack([x_ch, x_wd])\n",
    "x_train=scipy.sparse.csr_matrix(x)\n",
    "\n",
    "#开发数据\n",
    "\n",
    "y_val_= pd.Series(val_df['label'])\n",
    "\n",
    "x_val_wd = word_vectorizer.transform(x_val_)\n",
    "\n",
    "x_val_ch = char_vectorizer.transform(x_val_)\n",
    "x_val = hstack([x_val_ch, x_val_wd])\n",
    "x_val=scipy.sparse.csr_matrix(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#training\n",
    "\n",
    "print(\"LinearSVC with L1-based feature selection ,BoW(word-level)\")\n",
    "clf = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                    tol=1e-3))),\n",
    "    ('classification', LinearSVC(penalty=\"l2\"))])\n",
    "clf.fit(x_train, y_train_)\n",
    "joblib.dump(clf,f'{pkls}svm_website-app.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_result(actual, pred):\n",
    "    print('predict info:')\n",
    "    print('f1-score:{0:.3f}'.format(metrics.f1_score(actual, pred,average='macro',labels=np.unique(pred))))\n",
    "    print(\"accuracy:   %0.3f\" % metrics.accuracy_score(actual, pred))\n",
    "    print(metrics.classification_report(actual,pred,target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在开发集上测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(x_val)\n",
    "evaluation_result(y_val_, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 在线测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import jieba\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "def onlineTest(raw_query):\n",
    "    text = [''.join([w for w in jieba.cut(raw_query)])]\n",
    "    char_vectorizer=joblib.load(f'{pkls}vocab-ch_website-app.pkl')\n",
    "    test_ch = char_vectorizer.transform(text)\n",
    "    word_vectorizer=joblib.load(f'{pkls}vocab-wd_website-app.pkl')\n",
    "    test_wd = word_vectorizer.transform(text)\n",
    "    test_vec=hstack([test_ch, test_wd])\n",
    "    test_vec=csr_matrix(test_vec)\n",
    "\n",
    "    clf=joblib.load(f'{pkls}svm_website-app.pkl')\n",
    "    pred=clf.predict(test_vec)\n",
    "    return pred.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlineTest('中央台现在在放什么')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
